---
title: "Human Activity Recognition classification"
author: "H. Vicente"
date: "Sunday, September 21, 2014"
output: html_document
---

# Introduction
Automated Human Activity Recognition (HAR) is a topic that has raised its profile in the recent years. This is due to the generalisation of the use of sensors and the massive improvement in computing power leading to many successful use of machine learning to analyse this ever growing amount of data.

# Objective
The objective of this work is to analyse the data produced by 4 sensors (forearm, arm, belt, dumbbell) during weight lifting exercises in order to automatically classify if the the exercise has been carried out correctly or if common mistakes were made. This will be used as an assignment for the course Practical Machine Learning which will require to train a machine learning algorithm and then to predict the classe of a testing dataset.

# Data
This document uses the HAR data from Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.Read more:<http://groupware.les.inf.puc-rio.br/har#ixzz3DyigSno6>.
To train the classifier, a dataset has been generated by participants. Some would carry out the exercises respecting precisely the specifications, some would make common mistakes.
The training data was taken from: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>. This should be used to train a machine learning algorithm and predict the classes from the testing data : <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

# Analysis
## Data loading
Let's first load the data.

```{r}
dtTrain<-read.csv("C:/HERVE/Downloads/Practical Machine Learning/assignments/data/pml-training.csv")
dtTest<-read.csv("C:/HERVE/Downloads/Practical Machine Learning/assignments/data/pml-testing.csv")
```
The testing dataset dtTest will only be used to predict the classes for the assignment. The dataset dtTrain will be used to train, validate and test our machine learning algorithm.

## Data spliting
To make sure that the analysis is not biaised by some prior knowledge of the testing dataset, will will first split the dataset into 3 sets:

- training: 60% of the observation
- validating: 20% of obs.
- testing: 20% of obs.

We will use the package caret and set a seed to allow reproducible results.
```{r}
library(caret,warn.conflicts = F,quietly = T)
set.seed(3433)
inTrain = createDataPartition(dtTrain$classe,p = 0.60,list=FALSE)
training = dtTrain[ inTrain,]
testing = dtTrain[-inTrain,]
inValidation = createDataPartition(testing$classe,p = 0.5,list=FALSE)
validating= testing[inValidation,]
testing = testing[-inValidation,]
```

## Data exploration and cleaning
Let's first have a look at the dataset:
```{r,results="hide"}
summary(training)
str(training)
```

We can see that training data contains 11776 observations of 160 variables. We need first to identify the data that is not relevant or not good enough for the analysis such as:

- metadata: participant name, time of the exercise...
- variables with more than 90% missing values
- variables with near zero variance

To do so we will use the package caret plus some code to identify the variables to remove from the analysis.
```{r,result="hide"}
#Identify near zero variance observations
nsv<-nearZeroVar(training,saveMetrics = T)
#Identifying data 
manyNA<-apply(training,2,function(x){(sum(is.na(x))/length(x))>0.9})
training.clean<-training[,!(nsv$nzv | manyNA)]
#Removing metadata in columns 1 to 5
training.clean<-training.clean[,-(1:5)]

#Let's apply the same rules to the other datasets
validating.clean<-validating[,!(nsv$nzv | manyNA)]
validating.clean<-validating.clean[,-(1:5)]
testing.clean<-testing[,!(nsv$nzv | manyNA)]
testing.clean<-testing.clean[,-(1:5)]
```

We now have 53 variables to develop our algorithm, the 54th is the classe variable.

## Approach
We are faced with a classification problem. We will test 3 algorithms that are known for their good performance of this type of tasks:

- Random Forests (RF)
- Support Vector Machines (SVM)
- classification tree models using Quinlan's C5.0 algorithm (C50)

We will use the training data to train each model and use adaptative cross-validation to tune each model parameters. The validating data will be used to compare the performance of each model and select the best one. Then the testing data will be used on the best model to estimate the out of sample error.

### Random Forest
In this section we expore the use of RF. Because of the size of the dataset, the number of trees has been limited to 50 to reduce the computational time. Column 54 contains the classe variable. We use the confusion matrix to assess the performance of the model on the validating data
```{r,cache=TRUE}
set.seed(3433)
ctrl <- trainControl(method = "adaptive_cv", 
                     repeats = 5,
                     verboseIter = F)
rfModel<-train(training.clean[,-54],training.clean[,54],method="rf",trControl = ctrl,ntree=50,tuneLength = 4 )
rfCM<-confusionMatrix(validating$classe,predict(rfModel,validating.clean[,-54]))
print(rfCM)
```
The best RF model was obtained for a tuning value of mtry = 19.
From the confusion matrix we can see that the accuracy of the RF model is `r rfCM$overall[1]`

### Support Vector Machines
In this section we use the same procedure to expore the performance SVM. It is to be notted that SVM models require data normalisation to perform best. Note it is necessary to scale and center the data using the caret pre-processing functions.
```{r,cache=TRUE}
set.seed(3433)
ctrl <- trainControl(method = "adaptive_cv", 
                     repeats = 5,
                     verboseIter = F)
svmModel<-train(training.clean[,-54],training.clean[,54],method="svmRadial",tuneLength=9,preProc=c("center","scale"),trControl = ctrl)
svmCM<-confusionMatrix(validating$classe,predict(svmModel,validating.clean[,-54]))
print(svmCM)
```
The tuning parameter 'sigma' was held constant at a value of 0.01355. The best SVM model was obtained for a tuning value of sigma = 0.01355 and C = 64.
From the confusion matrix we can see that the accuracy of the SVM model is `r svmCM$overall[1]`

### C5.0 Classification trees
In this section we use the same procedure to expore the performance of RF. Data normalisation is not necessary.
```{r,cache=TRUE}
set.seed(3433)
ctrl <- trainControl(method = "adaptive_cv", 
                     repeats = 5,
                     verboseIter = F)
c5Model<-train(training.clean[,-54],training.clean[,54],method="C5.0",trControl = ctrl)
C50CM<-confusionMatrix(validating.clean$classe,predict(c5Model,validating.clean[,-54]))
print(C50CM)
```
The best C50 model was obtained for the tuning value of trials = 20, model = rules and winnow = TRUE.
From the confusion matrix we can see that the accuracy of the classification trees model is `r C50CM$overall[1]`

### Selection of best model
We can now select the best model using the reported confusion matrix accuracies on the validation dataset. We can see that the C50 model is the most accurate at predicting the validation set with an accuracy of 0.9985.

### Out of sample error
Now that the best model is selected, we can then estimate the out of sample error rate using the testing dataset. In fact since this dataset has not been used at all in the algorithm training or selection, the error on that dataset is considered to be the best estimator of the out of sample dataset.
```{r,cache=TRUE}
bestCM<-confusionMatrix(testing.clean$classe,predict(c5Model,testing.clean[,-54]))
print(bestCM)
```
From the confusion matrix we can see that the expected out of sample accuracy is `r bestCM$overall[1]`

### Prediction of the testing dataset
Now let's predict the classe of the test dataset provided.
```{r,warning=FALSE,quietly=T}
answers<-predict(c5Model,dtTest)
print(answers)
```


# Results
All 3 models perform very well. The C5.0 classification tree appears marginally better.


# Conclusions
This report shows how its is possible to apply machine learning techniques using the caret package to learn how to perform accurately Human Activity Recognition task.
